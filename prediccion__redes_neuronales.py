# -*- coding: utf-8 -*-
"""Prediccion _Redes_Neuronales.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qeJYOaZHoEs40kKz7yuxK18-oH5WUtIj

# Predicción sobre AL2-FIUNA


---


Estudiantes:

*   Marcos Ibañez
*   Hugo Melgarejo

# Importamos las librerias necesarias
"""

#Importamos las librerias de pandas,numpy,matplotlib y sklearn
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
#Sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import statsmodels.api as sm
import statsmodels.formula.api as smf
from statsmodels.stats.weightstats import ttest_ind
import seaborn as sns
from math import ceil
#Keras
import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout

"""# Importamos nuestra base de datos desde Github"""

url = 'https://raw.githubusercontent.com/diegostaPy/cursoIA/main/datosRendimiento/datosfiltrados.csv'
df = pd.read_csv(url)

df.head()

"""# Realizamos el pre-procesamiento de los datos

Reemplazamos la entrada aprobado que tiene valores S y N a 1 y 0, para realizar la predicción.
"""

df['Aprobado'] = df['Aprobado'].replace(['S','N'],[1,0])
df.head()

"""Separamos los campos referentes a la materia de interes."""

df = df[df['Asignatura'] == 'ALGEBRA LINEAL 2']
df.head()

df = df[['Primer.Par','Segundo.Par','Aprobado']]
df

"""A partir de ahora separamos los datos de interes como datos y etiquetas.

Datos
*   Primer Parcial
*   Segundo Parcial


Etiquetas:

*   Aprobado




"""

X = df[['Primer.Par','Segundo.Par']].values
Y = df[['Aprobado']].values

"""Ralizamos un gráfico para observar la situación y notamos que es posible realizar una frontera para clasificar a aprobados y reprobados, aunque la misma no será exacta siempre."""

plt.scatter(X[:,0],X[:,1],s=40,c=Y,cmap=plt.cm.Spectral)
plt.show

"""Separamos los datos en entrenamiento y test."""

X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,random_state = 1234,shuffle = True)

"""Escalamos los datos"""

from sklearn.preprocessing import StandardScaler
escalar = StandardScaler()
X_train = escalar.fit_transform(X_train)
X_test = escalar.transform(X_test)

"""# Creamos la red neuronal con Keras

Ahora creamos la red neuronal.
"""

model = Sequential()
model.add(Dense(2, activation='relu',input_shape = (2,),name = "Capa_de_Entrada"))
model.add(Dense(30,activation='relu', name="Capa_Oculta"))
model.add(Dense(1,activation = 'sigmoid',name = "Capa_de_Salida"))

"""Configuramos el modelo"""

model.compile(loss = 'binary_crossentropy',optimizer='adam')
model.summary()

history = model.fit(X_train, Y_train, epochs=2500, batch_size=32, verbose=1, validation_split=0.2)
model.summary()
print(history.history.keys())
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper right')
plt.show()

"""Comenzamos a entrenar el modelo

Evaluamos el error del modelo
"""

print(model.evaluate(X, Y))
print(model.metrics_names)

"""Imprimimos la matriz de confusión"""

from sklearn.metrics import confusion_matrix, classification_report  
predictions = model.predict(X_test)

predict_label = predictions.reshape(-1).round()

print('Cantidad de predicciones = {} \n'.format(predict_label.shape))

cf_matrix = confusion_matrix(Y_test, predict_label)
print(cf_matrix)

"""Graficamos la matriz de confusión."""

import seaborn as sns
sns.heatmap(cf_matrix, annot=True)

sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, 
            fmt='.2%', cmap='Blues')

"""# Creamos el reporte del modelo"""

report = classification_report(Y_test, predict_label)  
print(report)



